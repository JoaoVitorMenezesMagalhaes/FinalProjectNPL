{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant pages for the question:\n",
      "Page 169: Score 4664\n",
      "Page 172: Score 4396\n",
      "Page 120: Score 4389\n",
      "Page 82: Score 4307\n",
      "Page 165: Score 4305\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Initialize models for sentence and word filtering\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "word_model_name = 'bert-base-uncased'  # Choose a smaller BERT model for word filtering\n",
    "word_tokenizer = AutoTokenizer.from_pretrained(word_model_name)\n",
    "word_model = AutoModel.from_pretrained(word_model_name)\n",
    "\n",
    "def load_book(file_path):\n",
    "    \"\"\"\n",
    "    Load the book from a .txt file and extract text by page.\n",
    "    \"\"\"\n",
    "    book = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        pages = content.split('--- Página ')\n",
    "        for page in pages:\n",
    "            if page.strip():\n",
    "                match = re.match(r'(\\d+)', page)\n",
    "                if match:\n",
    "                    page_number = int(match.group(1))\n",
    "                    page_text = page[len(match.group(0)):].strip()\n",
    "                    book[page_number] = page_text\n",
    "    return book\n",
    "\n",
    "def sentence_level_filtering(sentences, question):\n",
    "    \"\"\"\n",
    "    Filters sentences based on their relevance to the question using PageRank.\n",
    "    \"\"\"\n",
    "    sentence_embeddings = sentence_model.encode(sentences, convert_to_tensor=True)\n",
    "    question_embedding = sentence_model.encode(question, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute pairwise similarity between sentences and the question\n",
    "    similarities = util.pytorch_cos_sim(sentence_embeddings, question_embedding).squeeze(1).tolist()\n",
    "    \n",
    "    # Construct a graph where nodes are sentences and edges are similarities\n",
    "    G = nx.Graph()\n",
    "    for i, sim in enumerate(similarities):\n",
    "        G.add_node(i, weight=sim)\n",
    "        for j in range(i + 1, len(sentences)):\n",
    "            sim_ij = util.pytorch_cos_sim(sentence_embeddings[i], sentence_embeddings[j]).item()\n",
    "            if sim_ij > 0.3:\n",
    "                G.add_edge(i, j, weight=sim_ij)\n",
    "    \n",
    "    # Apply PageRank\n",
    "    pagerank_scores = nx.pagerank(G, weight='weight')\n",
    "    \n",
    "    # Rank sentences by PageRank scores\n",
    "    ranked_sentences = sorted([(i, pagerank_scores[i]) for i in pagerank_scores], key=lambda x: x[1], reverse=True)\n",
    "    return [sentences[i] for i, _ in ranked_sentences]\n",
    "\n",
    "def word_level_filtering(sentence, question):\n",
    "    \"\"\"\n",
    "    Filters words within a sentence based on their relevance to the question using attention and PageRank.\n",
    "    \"\"\"\n",
    "    inputs = word_tokenizer(sentence, question, return_tensors='pt', truncation=True)\n",
    "    outputs = word_model(**inputs, output_attentions=True)\n",
    "    \n",
    "    # Extract attention weights\n",
    "    attention = torch.mean(torch.stack(outputs.attentions), dim=1)  # Average across layers\n",
    "    \n",
    "    # Compute PageRank on word graph\n",
    "    num_words = attention.size(-1)\n",
    "    adj_matrix = attention[0, :, :, :].mean(dim=0).detach().numpy()\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(num_words):\n",
    "        for j in range(num_words):\n",
    "            if adj_matrix[i, j] > 0.01:\n",
    "                G.add_edge(i, j, weight=adj_matrix[i, j])\n",
    "    pagerank_scores = nx.pagerank(G, weight='weight')\n",
    "    \n",
    "    token_ids = inputs['input_ids'][0].numpy()\n",
    "    tokens = word_tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    ranked_words = sorted([(tokens[i], pagerank_scores[i]) for i in range(len(tokens))], key=lambda x: x[1], reverse=True)\n",
    "    return [word for word, score in ranked_words if word not in ['[CLS]', '[SEP]']]\n",
    "\n",
    "def find_best_pages(question, book, top_n=3):\n",
    "    \"\"\"\n",
    "    Matches the question to the most relevant book pages using sentence and word filtering.\n",
    "    \"\"\"\n",
    "    page_scores = []\n",
    "    \n",
    "    for page_number, page_text in book.items():\n",
    "        # Step 1: Split page into sentences\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', page_text)\n",
    "        \n",
    "        # Step 2: Apply sentence-level filtering\n",
    "        relevant_sentences = sentence_level_filtering(sentences, question)\n",
    "        \n",
    "        # Step 3: Apply word-level filtering\n",
    "        total_score = 0\n",
    "        for sentence in relevant_sentences[:3]:  # Limit to top 3 sentences per page\n",
    "            filtered_words = word_level_filtering(sentence, question)\n",
    "            total_score += sum([len(word) for word in filtered_words])  # Simple scoring\n",
    "        \n",
    "        page_scores.append((page_number, total_score))\n",
    "    \n",
    "    # Rank pages by their total score\n",
    "    ranked_pages = sorted(page_scores, key=lambda x: x[1], reverse=True)\n",
    "    return ranked_pages[:top_n]\n",
    "\n",
    "# Example Usage\n",
    "file_path = 'transcricao_livro_ajustada.txt'\n",
    "book = load_book(file_path)\n",
    "\n",
    "question = 'Nos séculos XIV-XV, a sociedade feudal experimentou uma grave crise geral, que abalou profundamente as estruturas que sustentavam essa sociedade, abrindo espaços para a criação de relações capitalistas no interior das sociedades européias. Os efeitos da depressão dos séculos XIV-XV sobre a sociedade européia foram os seguintes, EXCETO: a expansão marítima dos séculos XV e XVI, rompendo os estreitos limites do comércio medieval. a centralização do poder nas mãos do rei, em contrapartida ao poder pulverizado dos senhores feudais. o surgimento de uma nova cultura mais urbana e laica, em oposição à rural-religiosa do feudalismo. a busca de urna nova espiritualidade, possibilitando a ruptura da unidade cristã através da Reforma. a ocupação do poder político pela burguesia, sustentada no crescente enriquecimento dessa classe.'\n",
    "\n",
    "top_pages = find_best_pages(question, book, top_n=5)\n",
    "\n",
    "print(\"Most relevant pages for the question:\")\n",
    "for page_number, score in top_pages:\n",
    "    print(f\"Page {page_number}: Score {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_find_best_pages(question, book, top_n=3):\n",
    "    \"\"\"\n",
    "    Matches the question to the most relevant book pages using improved filtering and scoring.\n",
    "    \"\"\"\n",
    "    page_scores = []\n",
    "\n",
    "    for page_number, page_text in book.items():\n",
    "        # Step 1: Split page into sentences\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', page_text)\n",
    "\n",
    "        # Step 2: Apply enhanced sentence-level filtering\n",
    "        sentence_embeddings = sentence_model.encode(sentences, convert_to_tensor=True)\n",
    "        question_embedding = sentence_model.encode(question, convert_to_tensor=True)\n",
    "\n",
    "        # Compute relevance scores\n",
    "        sentence_relevance_scores = util.pytorch_cos_sim(sentence_embeddings, question_embedding).squeeze(1).tolist()\n",
    "        ranked_sentences = sorted(zip(sentences, sentence_relevance_scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Select top sentences based on relevance\n",
    "        top_sentences = [sentence for sentence, score in ranked_sentences[:3]]\n",
    "\n",
    "        # Step 3: Apply word-level filtering\n",
    "        total_score = 0\n",
    "        for sentence in top_sentences:\n",
    "            inputs = word_tokenizer(sentence, question, return_tensors='pt', truncation=True)\n",
    "            outputs = word_model(**inputs, output_attentions=True)\n",
    "\n",
    "            # Extract attention weights\n",
    "            attention = torch.mean(torch.stack(outputs.attentions), dim=1)\n",
    "            num_words = attention.size(-1)\n",
    "            adj_matrix = attention[0, :, :, :].mean(dim=0).detach().numpy()\n",
    "\n",
    "            # Build word graph for PageRank\n",
    "            G = nx.DiGraph()\n",
    "            for i in range(num_words):\n",
    "                for j in range(num_words):\n",
    "                    if adj_matrix[i, j] > 0.01:\n",
    "                        G.add_edge(i, j, weight=adj_matrix[i, j])\n",
    "            pagerank_scores = nx.pagerank(G, weight='weight')\n",
    "\n",
    "            # Calculate relevance score for the sentence\n",
    "            token_ids = inputs['input_ids'][0].numpy()\n",
    "            tokens = word_tokenizer.convert_ids_to_tokens(token_ids)\n",
    "            ranked_words = sorted([(tokens[i], pagerank_scores[i]) for i in range(len(tokens)) if tokens[i] not in ['[CLS]', '[SEP]']],\n",
    "                                  key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # Add relevance scores for top words\n",
    "            total_score += sum(score for _, score in ranked_words[:5])  # Limit to top 5 words\n",
    "\n",
    "        # Append the total score for the page\n",
    "        page_scores.append((page_number, total_score))\n",
    "\n",
    "    # Rank pages by their total score\n",
    "    ranked_pages = sorted(page_scores, key=lambda x: x[1], reverse=True)\n",
    "    return ranked_pages[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(150, 0.24991501999010207),\n",
       " (165, 0.2480277251590664),\n",
       " (171, 0.23838443811754373)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced_find_best_pages(question, book, top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
