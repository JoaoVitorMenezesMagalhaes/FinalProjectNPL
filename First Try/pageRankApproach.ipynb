{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant pages for the question:\n",
      "Page 71: Score 0.6033105656189951\n",
      "Page 120: Score 0.4943276142988639\n",
      "Page 135: Score 0.4682799600187311\n",
      "Page 92: Score 0.4641684977996952\n",
      "Page 145: Score 0.41812834728695436\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "\n",
    "pt_stp_words = stopwords.words('portuguese')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word.lower() not in pt_stp_words])\n",
    "\n",
    "# Initialize models for sentence and word filtering\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2') # paraphrase-multilingual-MiniLM-L12-v2\n",
    "word_model_name = 'bert-base-uncased'  # Choose a smaller BERT model for word filtering\n",
    "word_tokenizer = AutoTokenizer.from_pretrained(word_model_name)\n",
    "word_model = AutoModel.from_pretrained(word_model_name)\n",
    "\n",
    "def load_book(file_path):\n",
    "    \"\"\"\n",
    "    Load the book from a .txt file and extract text by page.\n",
    "    \"\"\"\n",
    "    book = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        pages = content.split('--- Página ')\n",
    "        for page in pages:\n",
    "            if page.strip():\n",
    "                match = re.match(r'(\\d+)', page)\n",
    "                if match:\n",
    "                    page_number = int(match.group(1))\n",
    "                    page_text = page[len(match.group(0)):].strip()\n",
    "                    book[page_number] = page_text\n",
    "    return book\n",
    "\n",
    "def extract_keywords(question):\n",
    "    \"\"\"\n",
    "    Extracts main keywords from the question using synonyms and related terms.\n",
    "    \"\"\"\n",
    "    question_keywords = set(question.lower().split())\n",
    "    synonyms = set()\n",
    "    for word in question_keywords:\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name().lower())\n",
    "    return question_keywords.union(synonyms)\n",
    "\n",
    "def sentence_level_filtering(sentences, question):\n",
    "    \"\"\"\n",
    "    Filters sentences based on their relevance to the question using cosine similarity.\n",
    "    \"\"\"\n",
    "    sentence_embeddings = sentence_model.encode(sentences, convert_to_tensor=True)\n",
    "    question_embedding = sentence_model.encode(question, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute relevance scores using cosine similarity\n",
    "    relevance_scores = util.pytorch_cos_sim(sentence_embeddings, question_embedding).squeeze(1).tolist()\n",
    "    \n",
    "    # Dynamic thresholding based on variability in similarity scores\n",
    "    threshold = max(0.3, 0.5 * (max(relevance_scores) - min(relevance_scores)))\n",
    "    \n",
    "    # Rank sentences by relevance\n",
    "    ranked_sentences = sorted(\n",
    "        [(sentences[i], relevance_scores[i]) for i in range(len(sentences)) if relevance_scores[i] >= threshold],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    return ranked_sentences\n",
    "\n",
    "def word_level_filtering(sentence, question):\n",
    "    \"\"\"\n",
    "    Filters words within a sentence based on their relevance to the question using attention and PageRank.\n",
    "    \"\"\"\n",
    "    inputs = word_tokenizer(sentence, question, return_tensors='pt', truncation=True)\n",
    "    outputs = word_model(**inputs, output_attentions=True)\n",
    "    \n",
    "    # Extract attention weights\n",
    "    attention = torch.mean(torch.stack(outputs.attentions), dim=1)  # Average across layers\n",
    "    \n",
    "    # Compute PageRank on word graph\n",
    "    num_words = attention.size(-1)\n",
    "    adj_matrix = attention[0, :, :, :].mean(dim=0).detach().numpy()\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(num_words):\n",
    "        for j in range(num_words):\n",
    "            if adj_matrix[i, j] > 0.01:\n",
    "                G.add_edge(i, j, weight=adj_matrix[i, j])\n",
    "    pagerank_scores = nx.pagerank(G, weight='weight')\n",
    "    \n",
    "    token_ids = inputs['input_ids'][0].numpy()\n",
    "    tokens = word_tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    ranked_words = sorted(\n",
    "        [(tokens[i], pagerank_scores[i]) for i in range(len(tokens)) if tokens[i] not in ['[CLS]', '[SEP]']],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    return ranked_words  # Returns tuples of (word, score)\n",
    "\n",
    "def find_best_pages(question, book, top_n=3):\n",
    "    \"\"\"\n",
    "    Matches the question to the most relevant book pages using enhanced filtering and scoring.\n",
    "    \"\"\"\n",
    "    question_keywords = extract_keywords(question)  # Extract main keywords\n",
    "    page_scores = []\n",
    "\n",
    "    for page_number, page_text in book.items():\n",
    "        # Step 1: Split page into sentences\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', page_text)\n",
    "        \n",
    "        # Step 2: Apply sentence-level filtering\n",
    "        ranked_sentences = sentence_level_filtering(sentences, question)\n",
    "        top_sentences = ranked_sentences[:3]  # Limit to top 3 sentences for relevance\n",
    "        \n",
    "        # Step 3: Apply word-level filtering\n",
    "        total_score = 0\n",
    "        for sentence, sent_score in top_sentences:\n",
    "            ranked_words = word_level_filtering(sentence, question)\n",
    "            keyword_overlap = sum(1 for word, _ in ranked_words if word in question_keywords)\n",
    "            word_score = sum(score for _, score in ranked_words[:5]) + keyword_overlap  # Boost with keyword overlap\n",
    "            total_score += sent_score * word_score  # Combine sentence and word scores\n",
    "        \n",
    "        # Normalize by the number of sentences to prevent bias\n",
    "        normalized_score = total_score / len(sentences) if sentences else 0\n",
    "        page_scores.append((page_number, normalized_score))\n",
    "    \n",
    "    # Rank pages by their total score\n",
    "    ranked_pages = sorted(page_scores, key=lambda x: x[1], reverse=True)\n",
    "    return ranked_pages[:top_n]\n",
    "\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "file_path = 'transcricao_livro_ajustada.txt'\n",
    "book = load_book(file_path)\n",
    "\n",
    "question = 'A palavra “feudalismo” carrega consigo vários sentidos. Dentre eles, podem-se apontar aqueles ligados a: sociedades marcadas por dependências mútuas e assimétricas entre senhores e vassalos. relações de parentesco determinadas pelo local de nascimento, sobretudo quando urbano. regimes inteiramente dominados pela fé religiosa, seja ela cristã ou muçulmana. altas concentrações fundiárias e capitalistas. formas de economias de subsistência pré-agrícolas.'\n",
    "question = remove_stopwords(question)\n",
    "\n",
    "top_pages = find_best_pages(question, book, top_n=5)\n",
    "\n",
    "print(\"Most relevant pages for the question:\")\n",
    "for page_number, score in top_pages:\n",
    "    print(f\"Page {page_number}: Score {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
