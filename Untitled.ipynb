{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "525c689e-3f12-4587-a24d-dfa32b82fce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rafae\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rafae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\rafae\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rafae\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Páginas Relevantes: [118, 15, 117, 147, 109]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Baixar o conjunto de stopwords do NLTK (se necessário)\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Função para pré-processamento: remove stop words e caracteres especiais\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove pontuação\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Carregar o modelo apropriado para português\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Carregar o texto e dividir em páginas\n",
    "with open(\"transcricao_livro_ajustada.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "pages = text.split('--- Página ')[1:]  # Dividir por páginas\n",
    "page_texts = {int(page.split('---')[0]): preprocess_text(page.split('---', 1)[1].strip()) for page in pages}\n",
    "\n",
    "# Gerar embeddings para cada página\n",
    "page_embeddings = {}\n",
    "for page_num, content in page_texts.items():\n",
    "    page_embeddings[page_num] = model.encode(content)\n",
    "\n",
    "# Processar a query\n",
    "query = \"\"\"Nos séculos XIV-XV, a sociedade feudal experimentou uma grave crise geral, que abalou profundamente as estruturas que sustentavam essa sociedade, abrindo espaços para a criação de relações capitalistas no interior das sociedades européias. Os efeitos da depressão dos séculos XIV-XV sobre a sociedade européia foram os seguintes, EXCETO: a expansão marítima dos séculos XV e XVI, rompendo os estreitos limites do comércio medieval. a centralização do poder nas mãos do rei, em contrapartida ao poder pulverizado dos senhores feudais. o surgimento de uma nova cultura mais urbana e laica, em oposição à rural-religiosa do feudalismo. a busca de urna nova espiritualidade, possibilitando a ruptura da unidade cristã através da Reforma. a ocupação do poder político pela burguesia, sustentada no crescente enriquecimento dessa classe.\"\"\"\n",
    "query = preprocess_text(query)  # Preprocessar a query\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "# Calcular a similaridade de cosseno entre a query e os embeddings das páginas\n",
    "similarities = {}\n",
    "for page_num, embedding in page_embeddings.items():\n",
    "    similarity = cosine_similarity([query_embedding], [embedding])[0][0]\n",
    "    similarities[page_num] = similarity\n",
    "\n",
    "# Ordenar as páginas por relevância\n",
    "sorted_pages = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Exibir as páginas mais relevantes\n",
    "top_pages = [page[0] for page in sorted_pages[:5]]  # Top 5 páginas\n",
    "print(\"Páginas Relevantes:\", top_pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c79b047d-9a05-490a-a3dc-8a2a5fc4bf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rafae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Páginas Relevantes: [117, 118, 116, 113, 147]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Baixar o conjunto de stopwords do NLTK (se necessário)\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Função para pré-processamento: remove stop words e caracteres especiais\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove pontuação\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Carregar o modelo apropriado para português\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Carregar o texto e dividir em páginas\n",
    "with open(\"transcricao_livro_ajustada.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "pages = text.split('--- Página ')[1:]  # Dividir por páginas\n",
    "page_texts = {int(page.split('---')[0]): preprocess_text(page.split('---', 1)[1].strip()) for page in pages}\n",
    "\n",
    "# Gerar embeddings para cada página\n",
    "page_embeddings = {}\n",
    "for page_num, content in page_texts.items():\n",
    "    page_embeddings[page_num] = model.encode(content)\n",
    "\n",
    "# Processar a query\n",
    "query = \"\"\"Uma das características a ser reconhecida no feudalismo europeu é: A sociedade feudal era semelhante ao sistema de castas. Os ideais de honra e fidelidade vieram das instituições dos hunos. Vilões e servos estavam presos a várias obrigações, entre elas, o pagamento anual de capitação, talha e banalidades. A economia do feudo era dinâmica, estando voltada para o comércio dos feudos vizinhos. As relações de produção eram escravocratas.\"\"\"\n",
    "query = preprocess_text(query)  # Preprocessar a query\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "# Calcular a similaridade de cosseno entre a query e os embeddings das páginas\n",
    "similarities = {}\n",
    "for page_num, embedding in page_embeddings.items():\n",
    "    similarity = cosine_similarity([query_embedding], [embedding])[0][0]\n",
    "    similarities[page_num] = similarity\n",
    "\n",
    "# Ordenar as páginas por relevância\n",
    "sorted_pages = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Exibir as páginas mais relevantes\n",
    "top_pages = [page[0] for page in sorted_pages[:5]]  # Top 5 páginas\n",
    "print(\"Páginas Relevantes:\", top_pages)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "736f7288-f70a-496a-8e0b-f287dc14917b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rafae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Páginas Relevantes: [26, 16, 152, 31, 117]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Baixar o conjunto de stopwords do NLTK (se necessário)\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Função para pré-processamento: remove stop words e caracteres especiais\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove pontuação\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Carregar o modelo apropriado para português\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Carregar o texto e dividir em páginas\n",
    "with open(\"transcricao_livro_ajustada.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "pages = text.split('--- Página ')[1:]  # Dividir por páginas\n",
    "page_texts = {int(page.split('---')[0]): preprocess_text(page.split('---', 1)[1].strip()) for page in pages}\n",
    "\n",
    "# Gerar embeddings para cada página\n",
    "page_embeddings = {}\n",
    "for page_num, content in page_texts.items():\n",
    "    page_embeddings[page_num] = model.encode(content)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Example Dataset Class\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
    "\n",
    "# Autoencoder Model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = torch.relu(self.encoder(x))\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Prepare Data\n",
    "embedding_array = np.array(list(page_embeddings.values()))  # Convert embeddings to numpy array\n",
    "dataset = EmbeddingDataset(embedding_array)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Train Model\n",
    "input_dim = embedding_array.shape[1]\n",
    "hidden_dim = 128  # Reduced dimension\n",
    "model = Autoencoder(input_dim, hidden_dim)\n",
    "criterion = nn.MSELoss()  # Reconstruction loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed = model(batch)\n",
    "        loss = criterion(reconstructed, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "# Processar a query\n",
    "query = \"\"\"A casa de Deus, que acreditam una, está, portanto, dividida em três: uns oram, outros combatem, outros, enfim, trabalham. Essas três partes que coexistem não suportam ser separadas; os serviços prestados por uma são a condição das obras das outras duas; cada uma por sua vez encarrega-se de aliviar o conjunto… Assim a lei pode triunfar e o mundo gozar da paz. A ideologia apresentada por Aldalberon de Laon foi produzida durante a Idade Média. Um objetivo de tal ideologia e um processo que a ela se opôs estão indicados, respectivamente, em: Justificar a dominação estamental / revoltas camponesas. Subverter a hierarquia social / centralização monárquica. Impedir a igualdade jurídica / revoluções burguesas. Controlar a exploração econômica / unificação monetária. Questionar a ordem divina / Reforma Católica.\"\"\"\n",
    "query = preprocess_text(query)  # Preprocessar a query\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "# Calcular a similaridade de cosseno entre a query e os embeddings das páginas\n",
    "similarities = {}\n",
    "for page_num, embedding in page_embeddings.items():\n",
    "    similarity = cosine_similarity([query_embedding], [embedding])[0][0]\n",
    "    similarities[page_num] = similarity\n",
    "\n",
    "# Ordenar as páginas por relevância\n",
    "sorted_pages = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Exibir as páginas mais relevantes\n",
    "top_pages = [page[0] for page in sorted_pages[:5]]  # Top 5 páginas\n",
    "print(\"Páginas Relevantes:\", top_pages)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67c39bda-9f4c-41d8-87a8-033c82d0cab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rafae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning embeddings...\n",
      "Epoch 1, Loss: 0.1172\n",
      "Epoch 2, Loss: 0.0833\n",
      "Epoch 3, Loss: 0.0680\n",
      "Epoch 4, Loss: 0.0617\n",
      "Epoch 5, Loss: 0.0565\n",
      "Epoch 6, Loss: 0.0520\n",
      "Epoch 7, Loss: 0.0475\n",
      "Epoch 8, Loss: 0.0434\n",
      "Epoch 9, Loss: 0.0412\n",
      "Epoch 10, Loss: 0.0377\n",
      "Páginas Relevantes: [118, 15, 111, 145, 109]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# Download stop words for Portuguese\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Preprocessing function to remove stop words and special characters\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Load pre-trained multilingual model\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Load text data and split into pages\n",
    "with open(\"transcricao_livro_ajustada.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "pages = text.split('--- Página ')[1:]  # Split by page marker\n",
    "page_texts = {int(page.split('---')[0]): preprocess_text(page.split('---', 1)[1].strip()) for page in pages}\n",
    "\n",
    "# Generate embeddings for each page\n",
    "page_embeddings = {}\n",
    "for page_num, content in page_texts.items():\n",
    "    page_embeddings[page_num] = model.encode(content)\n",
    "\n",
    "# Convert embeddings to numpy array\n",
    "embedding_array = np.array(list(page_embeddings.values()))\n",
    "\n",
    "# Dataset for fine-tuning\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
    "\n",
    "# Autoencoder model for fine-tuning\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = torch.relu(self.encoder(x))\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Prepare data and model\n",
    "input_dim = embedding_array.shape[1]\n",
    "hidden_dim = 128  # Dimension to reduce embeddings\n",
    "dataset = EmbeddingDataset(embedding_array)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model_autoencoder = Autoencoder(input_dim, hidden_dim)\n",
    "criterion = nn.MSELoss()  # Reconstruction loss\n",
    "optimizer = optim.Adam(model_autoencoder.parameters(), lr=1e-3)\n",
    "\n",
    "# Fine-tune embeddings\n",
    "print(\"Fine-tuning embeddings...\")\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed = model_autoencoder(batch)\n",
    "        loss = criterion(reconstructed, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Generate fine-tuned embeddings\n",
    "fine_tuned_embeddings = {}\n",
    "for page_num, original_embedding in page_embeddings.items():\n",
    "    input_tensor = torch.tensor(original_embedding, dtype=torch.float).unsqueeze(0)\n",
    "    fine_tuned_embeddings[page_num] = model_autoencoder.encoder(input_tensor).detach().numpy()[0]\n",
    "\n",
    "# Query processing\n",
    "query = \"\"\"Nos séculos XIV-XV, a sociedade feudal experimentou uma grave crise geral, que abalou profundamente as estruturas que sustentavam essa sociedade, abrindo espaços para a criação de relações capitalistas no interior das sociedades européias. Os efeitos da depressão dos séculos XIV-XV sobre a sociedade européia foram os seguintes, EXCETO: a expansão marítima dos séculos XV e XVI, rompendo os estreitos limites do comércio medieval. a centralização do poder nas mãos do rei, em contrapartida ao poder pulverizado dos senhores feudais. o surgimento de uma nova cultura mais urbana e laica, em oposição à rural-religiosa do feudalismo. a busca de urna nova espiritualidade, possibilitando a ruptura da unidade cristã através da Reforma. a ocupação do poder político pela burguesia, sustentada no crescente enriquecimento dessa classe.\"\"\"\n",
    "query = preprocess_text(query)\n",
    "query_embedding = model.encode(query)\n",
    "query_embedding = model_autoencoder.encoder(torch.tensor(query_embedding, dtype=torch.float).unsqueeze(0)).detach().numpy()[0]\n",
    "\n",
    "# Perform cosine similarity search\n",
    "similarities = {}\n",
    "for page_num, embedding in fine_tuned_embeddings.items():\n",
    "    similarity = cosine_similarity([query_embedding], [embedding])[0][0]\n",
    "    similarities[page_num] = similarity\n",
    "\n",
    "# Sort pages by similarity\n",
    "sorted_pages = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "top_pages = [page[0] for page in sorted_pages[:5]]  # Top 5 most relevant pages\n",
    "\n",
    "print(\"Páginas Relevantes:\", top_pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7417ac-7c4c-4964-a4f2-ec5a5cb0faa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
